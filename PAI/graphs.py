# -*- coding: utf-8 -*-
"""graphs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jQuEjUgsbSN8YJal1eHoyPHj4NxiHueD
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib as m
import matplotlib.pyplot as plt

from sklearn.metrics import ConfusionMatrixDisplay

from pylab import rcParams

import numpy as np

#plt.style.use('fivethirtyeight')

m.rcParams['xtick.labelsize'] = 12
m.rcParams['ytick.labelsize'] = 12
m.rcParams['axes.labelsize'] = 14

rcParams['figure.figsize'] = 16, 8

# %matplotlib inline

"""## EfficientNetB1"""

data_binary = {
    'accuracy': [
        0.741584837436676, 0.8482936024665833, 0.8982000946998596, 0.9301075339317322,
        0.958158016204834, 0.9623655676841736, 0.9730014204978943, 0.9744039177894592,
        0.9786115288734436, 0.9811828136444092, 0.9822347164154053, 0.9855072498321533,
        0.9848060011863708, 0.9872604012489319, 0.9872604012489319, 0.9956755638122559,
        0.9977793097496033, 0.9977793097496033, 0.9987143278121948],
    'loss': [
        0.520508885383606, 0.35807013511657715, 0.2550821900367737, 0.17956873774528503,
        0.11509905755519867, 0.10123327374458313, 0.07275141030550003, 0.06933014839887619,
        0.057657599449157715, 0.05415773764252663, 0.04748274013400078, 0.04352341219782829,
        0.04060220345854759, 0.03506765142083168, 0.03939560055732727, 0.011614019051194191,
        0.006003536283969879, 0.005179058760404587, 0.0034316617529839277],
    'val_accuracy': [
        0.827102780342102, 0.8883177638053894, 0.9182242751121521, 0.9420560598373413,
        0.9397196173667908, 0.9602803587913513, 0.9579439163208008, 0.9612149596214294,
        0.96355140209198, 0.9668224453926086, 0.9672897458076477, 0.9733644723892212,
        0.9742990732192993, 0.9462617039680481, 0.9710280299186707, 0.9831775426864624,
        0.9799065589904785, 0.982243001461029, 0.98177570104599],
    'val_loss': [
        0.4746735692024231, 0.2686018943786621, 0.22104883193969727, 0.1497720628976822,
        0.1606145203113556, 0.14233489334583282, 0.11704237759113312, 0.12741829454898834,
        0.1099051982164383, 0.12913037836551666, 0.12937989830970764, 0.09872478246688843,
        0.11336571723222733, 0.19928520917892456, 0.09611073136329651, 0.06442079693078995,
        0.08466356992721558, 0.07711535692214966, 0.08041348308324814],
    'learning_rate': [
        0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513,
        0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513,
        0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513,
        0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.00020000000949949026,
        0.00020000000949949026, 0.00020000000949949026, 4.0000002627493814e-05]}

data_multiclass = {'accuracy': [0.18424396216869354, 0.20393900573253632, 0.24904701113700867, 0.28653112053871155, 0.30432021617889404, 0.33926302194595337, 0.37039390206336975, 0.38246506452560425, 0.4269377291202545, 0.4396442174911499, 0.4593392610549927, 0.4923761188983917, 0.5139771103858948, 0.5146124362945557, 0.536848783493042, 0.5743328928947449, 0.5800508260726929, 0.5959339141845703, 0.616264283657074, 0.6080051064491272, 0.6454892158508301, 0.6556543707847595, 0.6759847402572632, 0.671537458896637, 0.6785260438919067], 'loss': [1.8308625221252441, 1.7992948293685913, 1.7496341466903687, 1.7065801620483398, 1.6723979711532593, 1.621558666229248, 1.5633771419525146, 1.5300668478012085, 1.4824670553207397, 1.4328354597091675, 1.3745473623275757, 1.3136974573135376, 1.2518393993377686, 1.2140064239501953, 1.166465401649475, 1.1127229928970337, 1.0705490112304688, 1.024048089981079, 0.9884747266769409, 0.9519008994102478, 0.9171072840690613, 0.8738874793052673, 0.8387035727500916, 0.8363142013549805, 0.8190916180610657], 'val_accuracy': [0.19796954095363617, 0.1928934007883072, 0.20558375120162964, 0.2157360464334488, 0.22081218659877777, 0.22335025668144226, 0.22081218659877777, 0.27664974331855774, 0.3071065843105316, 0.32487308979034424, 0.3502538204193115, 0.36548224091529846, 0.398477166891098, 0.4263959527015686, 0.4263959527015686, 0.48477157950401306, 0.5076141953468323, 0.5558375716209412, 0.5812183022499084, 0.624365508556366, 0.6598984599113464, 0.6472080945968628, 0.6548223495483398, 0.6548223495483398, 0.6548223495483398], 'val_loss': [1.7829782962799072, 1.772467851638794, 1.7714637517929077, 1.7687187194824219, 1.7592955827713013, 1.7427442073822021, 1.7176425457000732, 1.6857528686523438, 1.6532758474349976, 1.6130558252334595, 1.5569789409637451, 1.486951231956482, 1.4383432865142822, 1.3836737871170044, 1.3251062631607056, 1.2545967102050781, 1.199558973312378, 1.1339764595031738, 1.0662776231765747, 1.007638931274414, 0.9618471264839172, 0.9253007173538208, 0.8955450057983398, 0.8849287629127502, 0.8770154118537903], 'learning_rate': [1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 1.9999999494757503e-05, 3.999999989900971e-06, 3.999999989900971e-06]}

m_epochs_a = [str(i) for i in range(len(data_multiclass["accuracy"]))]
m_epochs_l = [str(i) for i in range(len(data_multiclass["loss"]))]

b_epochs_a = [str(i) for i in range(len(data_binary["accuracy"]))]
b_epochs_l = [str(i) for i in range(len(data_binary["loss"]))]

fig, axs = plt.subplots(2, 2, figsize=(25, 25))

# Binary accuracy curve

title_fontsize = 17
fontsize = 15
axs[0][0].plot(b_epochs_a, data_binary["accuracy"], linewidth=3, label="Acurácia de Treino")
axs[0][0].plot(b_epochs_a, data_binary["val_accuracy"], linewidth=3, label="Acurácia de Validação")
axs[0][0].plot([0, 1], [0.7, 1], alpha=0.0)

axs[0][0].set_title("Acurácia de Treino e Validação por Época na Classificação Binária", fontsize=title_fontsize)
axs[0][0].set_xlabel('Época', fontsize=fontsize)
axs[0][0].set_ylabel('Acurácia', fontsize=fontsize)
axs[0][0].legend(fontsize=title_fontsize)

# Binary loss curve

axs[0][1].plot(b_epochs_l, data_binary["loss"], linewidth=3, label="Perda de Treino")
axs[0][1].plot(b_epochs_l, data_binary["val_loss"], linewidth=3, label="Perda de Validação")

axs[0][1].set_title("Perda de Treino e Validação por Época na Classificação Binária", fontsize=title_fontsize)
axs[0][1].set_xlabel('Época', fontsize=fontsize)
axs[0][1].set_ylabel('Perda', fontsize=fontsize)
axs[0][1].legend(fontsize=title_fontsize)

# Multiclass accuracy curve

axs[1][0].plot(m_epochs_a, data_multiclass["accuracy"], linewidth=3, label="Acurácia de Treino")
axs[1][0].plot(m_epochs_a, data_multiclass["val_accuracy"], linewidth=3, label="Acurácia de Validação")
axs[1][0].plot([0, 1], [0.2, 0.8], alpha=0.0)

axs[1][0].set_title("Acurácia de Treino e Validação por Época na Classificação Multiclasse com 6 Classes", fontsize=title_fontsize)
axs[1][0].set_xlabel('Época', fontsize=fontsize)
axs[1][0].set_ylabel('Acurácia', fontsize=fontsize)
axs[1][0].legend(fontsize=title_fontsize)

# Multiclass loss curve

axs[1][1].plot(m_epochs_l, data_multiclass["loss"], linewidth=3, label="Perda de Treino")
axs[1][1].plot(m_epochs_l, data_multiclass["val_loss"], linewidth=3, label="Perda de Validação")

axs[1][1].set_title("Perda de Treino e Validação por Época na Classificação Multiclasse com 6 Classes", fontsize=title_fontsize)
axs[1][1].set_xlabel('Época', fontsize=fontsize)
axs[1][1].set_ylabel('Perda', fontsize=fontsize)
axs[1][1].legend(fontsize=title_fontsize)

plt.show()

classes = ["0", "1", "2", "3", "4", "5"]

cm_multiclass = np.array([[20, 0, 24, 3, 11, 1], [3, 66, 0, 0, 0, 4], [12, 0, 47, 4, 7, 0], [3, 0, 1, 52, 1, 13], [6, 4, 20, 1, 32, 1], [0, 0, 0, 1, 0, 57]])
cm_multiclass = cm_multiclass.astype('float') / cm_multiclass.sum(axis=1)[:, np.newaxis]

def generate_confusion_matrix(cm, inf, sup) -> None:
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[i for i in range(inf, sup)])
  disp.plot()

generate_confusion_matrix(cm_multiclass, 0, 6)

data_binary = {
    'accuracy': [
        0.741584837436676, 0.8482936024665833, 0.8982000946998596, 0.9301075339317322,
        0.958158016204834, 0.9623655676841736, 0.9730014204978943, 0.9744039177894592,
        0.9786115288734436, 0.9811828136444092, 0.9822347164154053, 0.9855072498321533,
        0.9848060011863708, 0.9872604012489319, 0.9872604012489319, 0.9956755638122559,
        0.9977793097496033, 0.9977793097496033, 0.9987143278121948],
    'loss': [
        0.520508885383606, 0.35807013511657715, 0.2550821900367737, 0.17956873774528503,
        0.11509905755519867, 0.10123327374458313, 0.07275141030550003, 0.06933014839887619,
        0.057657599449157715, 0.05415773764252663, 0.04748274013400078, 0.04352341219782829,
        0.04060220345854759, 0.03506765142083168, 0.03939560055732727, 0.011614019051194191,
        0.006003536283969879, 0.005179058760404587, 0.0034316617529839277],
    'val_accuracy': [
        0.827102780342102, 0.8883177638053894, 0.9182242751121521, 0.9420560598373413,
        0.9397196173667908, 0.9602803587913513, 0.9579439163208008, 0.9612149596214294,
        0.96355140209198, 0.9668224453926086, 0.9672897458076477, 0.9733644723892212,
        0.9742990732192993, 0.9462617039680481, 0.9710280299186707, 0.9831775426864624,
        0.9799065589904785, 0.982243001461029, 0.98177570104599],
    'val_loss': [
        0.4746735692024231, 0.2686018943786621, 0.22104883193969727, 0.1497720628976822,
        0.1606145203113556, 0.14233489334583282, 0.11704237759113312, 0.12741829454898834,
        0.1099051982164383, 0.12913037836551666, 0.12937989830970764, 0.09872478246688843,
        0.11336571723222733, 0.19928520917892456, 0.09611073136329651, 0.06442079693078995,
        0.08466356992721558, 0.07711535692214966, 0.08041348308324814],
    'learning_rate': [
        0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513,
        0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513,
        0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513,
        0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.00020000000949949026,
        0.00020000000949949026, 0.00020000000949949026, 4.0000002627493814e-05]}

epochs = [str(i) for i in range(len(data_binary["accuracy"]))]

plt.plot(epochs, data_binary["accuracy"], linewidth=3, label="Accuracy")
plt.plot(epochs, data_binary["val_accuracy"], linewidth=3, label="Validation Accuracy")

plt.title("Accuracy and Validation Accuracy per Epoch in Binary Classification", fontsize="xx-large")
plt.xlabel("Epoch", fontsize="x-large")
plt.ylabel("Accuracy", fontsize="x-large")

plt.legend()

plt.show()

plt.plot(epochs, data_binary["loss"], linewidth=3, label="Loss")
plt.plot(epochs, data_binary["val_loss"], linewidth=3, label="Validation Loss")

plt.title("Loss and Validation Loss per Epoch in Binary Classification", fontsize="xx-large")
plt.xlabel("Epoch", fontsize="x-large")
plt.ylabel("Loss", fontsize="x-large")

plt.legend()

plt.show()

classes = ["0", "1"]

cm_binary = np.array([[910, 159], [234, 837]])
cm_binary = cm_binary.astype('float') / cm_binary.sum(axis=1)[:, np.newaxis]

def generate_confusion_matrix(cm, inf, sup) -> None: #*
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[i for i in range(inf, sup)])
  disp.plot()

generate_confusion_matrix(cm_binary, inf=0, sup=2)

"""## SVM"""

classes = ["0", "1"]

cm_binary = np.array([[0.44485981, 0.0546729], [0.29953271, 0.20093458]])
cm_binary = cm_binary.astype('float') / cm_binary.sum(axis=1)[:, np.newaxis]

def generate_confusion_matrix(cm, inf, sup) -> None: #*
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[i for i in range(inf, sup)])
  disp.plot()

generate_confusion_matrix(cm_binary, inf=0, sup=2)

classes = ["0", "1", "2", "3", "4", "5"]

cm_multiclass = np.array([[0.03299492, 0.01522843, 0.05837563, 0.00761421, 0.03299492, 0.00253807], [0.00507614, 0.11675127, 0.01015228, 0.01015228, 0.04060914, 0.00253807], [0., 0.00253807, 0.12690355, 0.01015228, 0.03553299, 0.00253807], [0.00507614, 0.03045685, 0.02284264, 0.07614213, 0.02791878, 0.01522843], [0.0177665 , 0.00761421, 0.09390863, 0.00507614, 0.03553299, 0.00253807], [0.00253807, 0.03299492, 0.00761421, 0.00507614, 0.01522843, 0.08375635],])
cm_multiclass = cm_multiclass.astype('float') / cm_multiclass.sum(axis=1)[:, np.newaxis]

def generate_confusion_matrix(cm, inf, sup) -> None: #*
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[i for i in range(inf, sup)])
  disp.plot()

generate_confusion_matrix(cm_multiclass, 0, 6)
